{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from scipy.stats import skew, kurtosis\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from scipy.signal import resample\n",
    "\n",
    "# 필요한 상수 정의\n",
    "SAMPLE_RATE = 16000\n",
    "FOLDER_NAME = \"data\"  # csv파일 생성할 폴더 이름\n",
    "RESULTS_FILE_NAME = \"voyager_test_results\" # test 결과 파일\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_file():\n",
    "  file_name = open(f'202301ml_fmcc/fmcc_test_ref.txt', 'r')\n",
    "  test_data_list = []\n",
    "  label_list = []\n",
    "\n",
    "  for i in file_name.readlines():\n",
    "    file, label = i.strip('\\n').split(' ')\n",
    "    \n",
    "    f = open(f'202301ml_fmcc/raw16k/test/' + file + '.raw', 'rb')\n",
    "    file = np.fromfile(f, dtype='int16', sep=\"\")\n",
    "    \n",
    "    test_data_list.append(file.astype(np.float32))\n",
    "    label_list.append(0 if label[0] == 'm' else 1)\n",
    "    \n",
    "  return test_data_list, label_list\n",
    "\n",
    "def get_train_file():\n",
    "  file_name = open(f'202301ml_fmcc/fmcc_train.ctl', 'r')\n",
    "  train_data_list = []\n",
    "  label_list = []\n",
    "  for i in file_name.readlines():\n",
    "    i = i.strip('\\n')\n",
    "    f = open(f'202301ml_fmcc/raw16k/train/' + i + '.raw', 'rb')\n",
    "    file = np.fromfile(f, dtype='int16', sep=\"\")\n",
    "    \n",
    "    train_data_list.append(file.astype(np.float32))\n",
    "    label_list.append(0 if i[0] == 'M' else 1)\n",
    "        \n",
    "    # add_noise_audio = file.astype(np.float32)\n",
    "    # noise = np.random.normal(0, 500, add_noise_audio.shape)\n",
    "\n",
    "    #원본 음성 데이터에 노이즈 추가\n",
    "    # noisy_signal = add_noise_audio + noise\n",
    "    # train_data_list.append(noisy_signal)\n",
    "    # label_list.append(0 if i[0] == 'M' else 1)\n",
    "    \n",
    "    # extended_audio = file.astype(np.float32)\n",
    "    # train_data_list.append(resample(extended_audio, int(len(extended_audio) / 2)))\n",
    "    # label_list.append(0 if i[0] == 'M' else 1)\n",
    "    \n",
    "  return train_data_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(data): \n",
    "    ft1 = librosa.feature.mfcc(y=data, sr=SAMPLE_RATE, n_mfcc=20, fmin=0.0, fmax=500.0)\n",
    "    # ft2 = librosa.feature.zero_crossing_rate(y = data)[0]\n",
    "    # ft3 = librosa.feature.spectral_rolloff(y = data)[0]\n",
    "    # ft4 = librosa.feature.spectral_centroid(y = data)[0]\n",
    "    # ft5 = librosa.feature.spectral_contrast(y = data)[0]\n",
    "    # ft6 = librosa.feature.spectral_bandwidth(y = data)[0]\n",
    "    ft1_trunc = np.hstack((np.mean(ft1, axis=1), np.std(ft1, axis=1), skew(ft1, axis = 1), kurtosis(ft1, axis=1), np.max(ft1, axis=1), np.median(ft1, axis=1), np.min(ft1, axis=1)))\n",
    "    # ft2_trunc = np.hstack((np.mean(ft2), np.std(ft2), skew(ft2), np.max(ft2), np.median(ft2), np.min(ft2)))\n",
    "    # ft3_trunc = np.hstack((np.mean(ft3), np.std(ft3), skew(ft3), np.max(ft3), np.median(ft3), np.min(ft3)))\n",
    "    # ft4_trunc = np.hstack((np.mean(ft4), np.std(ft4), skew(ft4), np.max(ft4), np.median(ft4), np.min(ft4)))\n",
    "    # ft5_trunc = np.hstack((np.mean(ft5), np.std(ft5), skew(ft5), np.max(ft5), np.median(ft5), np.min(ft5)))\n",
    "    # ft6_trunc = np.hstack((np.mean(ft6), np.std(ft6), skew(ft6), np.max(ft6), np.median(ft6), np.max(ft6)))\n",
    "    # return pd.Series(np.hstack((ft1_trunc, ft2_trunc, ft3_trunc, ft4_trunc, ft5_trunc, ft6_trunc)))\n",
    "    return pd.Series(np.hstack((ft1_trunc,)))\n",
    "    \n",
    "def preprocess_data(data_list):\n",
    "    feature_list = []\n",
    "    for i in tqdm(range(len(data_list))):\n",
    "        feature_list.append(get_mfcc(data_list[i]))\n",
    "    return pd.DataFrame(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미 동일한 이름의 폴더가 존재합니다.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(FOLDER_NAME)\n",
    "    print(f\"{FOLDER_NAME} 폴더가 성공적으로 생성되었습니다.\")\n",
    "except FileExistsError:\n",
    "    print(\"이미 동일한 이름의 폴더가 존재합니다.\")\n",
    "except Exception as e:\n",
    "    print(\"폴더 생성 중 오류가 발생했습니다:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv 파일을 불러옵니다.\n",
      "데이터 로드 완료\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  print(\"csv 파일을 불러옵니다.\")\n",
    "  train_data_feature = pd.read_csv(f'{FOLDER_NAME}/train_data_feature.csv')\n",
    "  test_data_feature = pd.read_csv(f'{FOLDER_NAME}/test_data_feature.csv')\n",
    "  \n",
    "  train_label = pd.read_csv(f'{FOLDER_NAME}/train_data_label.csv')\n",
    "  test_label = pd.read_csv(f'{FOLDER_NAME}/test_data_label.csv')\n",
    "except:\n",
    "  print(\"csv 파일을 생성합니다.\")\n",
    "  train_data, train_label = get_train_file()\n",
    "  test_data, test_label = get_test_file()\n",
    "\n",
    "  train_data_feature = preprocess_data(train_data)\n",
    "  test_data_feature = preprocess_data(test_data)\n",
    "  \n",
    "  train_data_feature.to_csv(f'{FOLDER_NAME}/train_data_feature.csv', index=False)\n",
    "  test_data_feature.to_csv(f'{FOLDER_NAME}/test_data_feature.csv', index=False)\n",
    "  \n",
    "  pd.DataFrame(train_label).to_csv(f'{FOLDER_NAME}/train_data_label.csv', index=False)\n",
    "  pd.DataFrame(test_label).to_csv(f'{FOLDER_NAME}/test_data_label.csv', index=False)\n",
    "else:\n",
    "  train_label = train_label.values.ravel()\n",
    "  test_label = test_label.values.ravel()\n",
    "finally:\n",
    "  print(\"데이터 로드 완료\")\n",
    "  train_data_label = np.array(train_label)\n",
    "  test_data_label = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 140)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_feature.shape\n",
    "test_data_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaling for PCA\n",
    "scaler = StandardScaler()\n",
    "train_x_pca = scaler.fit_transform(train_data_feature)\n",
    "test_x_pca = scaler.fit_transform(test_data_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9808\n"
     ]
    }
   ],
   "source": [
    "# Fit an SVM model\n",
    "clf = SVC(kernel='rbf', probability=True)\n",
    "clf.fit(train_x_pca, train_data_label)\n",
    "\n",
    "train_accuracy = accuracy_score(clf.predict(train_x_pca), train_data_label)\n",
    "print(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9044444444444445\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = accuracy_score(clf.predict(test_x_pca), test_data_label)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 원래 mfcc, nosie 추가\n",
    "0.9022222222222223\n",
    "\n",
    "2. 원래 mfcc, 음성 길이 증가\n",
    "0.7577777777777778\n",
    "\n",
    "3. 원래 mfcc, 음성 길이 감소\n",
    "0.7433333333333333\n",
    "\n",
    "4. 수정된 mfcc, noise 추가\n",
    "0.9022222222222223\n",
    "\n",
    "5. 수정된 mfcc\n",
    "0.9044444444444445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = open('fmcc_test_ref.txt', 'r')\n",
    "test_file_name = []\n",
    "\n",
    "for i in file_name.readlines():\n",
    "    file, _ = i.strip('\\n').split(' ')\n",
    "    test_file_name.append(file)\n",
    "\n",
    "def write_predictions(predictions, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for index, pred in enumerate(predictions):\n",
    "            file.write((f'{test_file_name[index]} ' + ('feml' if pred == 1 else 'male')) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '202301ml_fmcc/voyager_test_results.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_predictions \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(test_x_pca)\n\u001b[0;32m----> 2\u001b[0m write_predictions(test_predictions, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m202301ml_fmcc/\u001b[39;49m\u001b[39m{\u001b[39;49;00mRESULTS_FILE_NAME\u001b[39m}\u001b[39;49;00m\u001b[39m.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m, in \u001b[0;36mwrite_predictions\u001b[0;34m(predictions, filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite_predictions\u001b[39m(predictions, filename):\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     10\u001b[0m         \u001b[39mfor\u001b[39;00m index, pred \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(predictions):\n\u001b[1;32m     11\u001b[0m             file\u001b[39m.\u001b[39mwrite((\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtest_file_name[index]\u001b[39m}\u001b[39;00m\u001b[39m.raw \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mfeml\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m pred \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mmale\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fmcc/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '202301ml_fmcc/voyager_test_results.txt'"
     ]
    }
   ],
   "source": [
    "test_predictions = clf.predict(test_x_pca)\n",
    "write_predictions(test_predictions, f'{RESULTS_FILE_NAME}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = svm.SVC(kernel='linear')\n",
    "\n",
    "# Compute the learning curve scores\n",
    "train_sizes, train_scores, test_scores = learning_curve(svm_classifier, X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "\n",
    "# Compute the mean and standard deviation of training scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Compute the mean and standard deviation of test scores\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure()\n",
    "plt.title(\"SVM Learning Curve\")\n",
    "plt.xlabel(\"Training Examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "\n",
    "# Plot the training scores with error bars\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training Score\")\n",
    "\n",
    "# Plot the cross-validation scores with error bars\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-Validation Score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
